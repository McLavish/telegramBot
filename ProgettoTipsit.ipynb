{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProgettoTipsit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/McLavish/telegramBot/blob/master/ProgettoTipsit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtwzZwAH0why",
        "colab_type": "text"
      },
      "source": [
        "# Programma WOW per generare testo con GTP-2 VELOCEMENTE :0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VfpPtOgFYGD",
        "colab_type": "code",
        "outputId": "a85ac540-b22e-465e-d72f-3e2567925871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "source": [
        "#Dependency per il machine learning\n",
        "!pip install torch torchvision\n",
        "!pip install transformers\n",
        "\n",
        "#modelli disponibili: https://huggingface.co/models?search=gpt2-"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bw3ZvvWlVyQS",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "\n",
        "class GeneratoreTesto:\n",
        "    def __init__(self):\n",
        "      seed = random.randint(0, 2147483647)\n",
        "      torch.random.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "      #imposto la GPU come dispositivo da utilizzare (La CPU è lenta)\n",
        "      self.device = \"cuda\"\n",
        "\n",
        "      #lista modelli: https://huggingface.co/models?search=gpt2-\n",
        "      model_name_or_path = \"gpt2-xl\"\n",
        "\n",
        "      self.tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "      self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "      #imposto modalità evaluation del modello (no training)\n",
        "      self.model.eval()\n",
        "      self.model.to(self.device)\n",
        "      \n",
        "    def generate_text(self,\n",
        "      prompt=\"\",\n",
        "      #lunghezza generazione\n",
        "      length=20,\n",
        "      #se devo fermarmi prima a leggere il testo generato\n",
        "      stop_token = None,\n",
        "      temperature=1.0,\n",
        "      repetition_penalty=1.0,\n",
        "      k=0,\n",
        "      p=0.9,\n",
        "      #quanti campioni voglio generare (io 1 alla volta)\n",
        "      num_return_sequences=1,\n",
        "    ):\n",
        "      #Prendo la lunghezza massima del modello\n",
        "      lunghezza_massima = self.model.config.max_position_embeddings\n",
        "\n",
        "      print(lunghezza_massima);\n",
        "      \n",
        "      #Controllo che la lunghezza richiesta + la lunghezza dell'input siano minori del modello\n",
        "      if length < 0 or length > lunghezza_massima:\n",
        "        length = lunghezza_massima\n",
        "\n",
        "      #Codifico il testo\n",
        "      encoded_prompt = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "      encoded_prompt = encoded_prompt.to(self.device)\n",
        "\n",
        "      #Genero la sequenza\n",
        "      output_sequences = self.model.generate (\n",
        "          input_ids=encoded_prompt,\n",
        "          max_length=length + len(encoded_prompt[0]),\n",
        "          temperature=temperature,\n",
        "          top_k=k,\n",
        "          top_p=p,\n",
        "          repetition_penalty=repetition_penalty,\n",
        "          do_sample=True,\n",
        "          num_return_sequences=num_return_sequences,\n",
        "          )\n",
        "      \n",
        "      #Prendo solo il primo batch (potrei generare più batch in parallelo ma al mio programma non serve)\n",
        "      generated_sequence = output_sequences[0]\n",
        "\n",
        "      # Decodifico il testo\n",
        "      text = self.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "      # Rimuovo il prompt iniziale dal testo (perchè ti ridà anche quello dopo che ha generato)\n",
        "      total_sequence = (text[len(prompt) :])\n",
        "\n",
        "      # Se è stato specificato uno stop token rimuovo tutto quello che c'è dopo di lui\n",
        "      text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "      return total_sequence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVhEbFr2wiZj",
        "colab_type": "code",
        "outputId": "a186dc7c-2e84-4c59-a75a-3b2da78047a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#Istanzio l'oggetto che mi genera il testo e faccio una prova (così carica in memoria il modello)\n",
        "\n",
        "generator = GeneratoreTesto()\n",
        "\n",
        "generator.generate_text(\n",
        "            length=50,\n",
        "            prompt=\"\"\"Hamlet was once a man of honor\"\"\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', that he never believed in or practised the love of riches. He died rich and happy.\\n\\nAs I glance upon his remains, my heart bleeds for the smallness of his life. He was only sixty years of age when he'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW8O3qUGFp9p",
        "colab_type": "code",
        "outputId": "d852d727-e0f2-47d4-d70a-5af363267562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Creo un tunnel con ngrok perchè Google Colab non permette la creazione di API esposte\n",
        "\n",
        "!if [ ! -f \"ngrok-stable-linux-amd64.zip\" ]; then wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip; fi\n",
        "!if [ ! -f \"ngrok\" ]; then unzip -o ngrok-stable-linux-amd64.zip; fi\n",
        "\n",
        "!./ngrok authtoken \"1aMSPEPk5WpPiquwebio1IWcILq_2RRMqrTUj968qnmrXss6L\"\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 5000 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "https://2db4373c.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AhLh5NU6uxo",
        "colab_type": "code",
        "outputId": "a32521dc-6f5e-4880-f0de-c0f9d4549be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "#Dependency per il web server con FastAPI\n",
        "\n",
        "!pip install fastapi\n",
        "!pip install uvicorn\n",
        "!pip install pydantic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.6/dist-packages (0.54.1)\n",
            "Requirement already satisfied: starlette==0.13.2 in /usr/local/lib/python3.6/dist-packages (from fastapi) (0.13.2)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=0.32.2 in /usr/local/lib/python3.6/dist-packages (from fastapi) (1.5.1)\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic<2.0.0,>=0.32.2->fastapi) (0.7)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.6/dist-packages (0.11.5)\n",
            "Requirement already satisfied: httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.6/dist-packages (from uvicorn) (0.1.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.6/dist-packages (from uvicorn) (0.9.0)\n",
            "Requirement already satisfied: uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.6/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: websockets==8.* in /usr/local/lib/python3.6/dist-packages (from uvicorn) (8.1)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.6/dist-packages (from uvicorn) (7.1.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic) (0.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnpHZhGk24Ov",
        "colab_type": "code",
        "outputId": "d4f3627d-4c0f-4909-b407-04487d453592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "class Richiesta(BaseModel):\n",
        "    prompt: str\n",
        "    length: int = None\n",
        "    stop_token: str = None\n",
        "    temperature: float = None\n",
        "    repetition_penalty: float = None\n",
        "    k: float = None\n",
        "    p: float = None\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "      return {\"string\": \"Hello World\"}\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "def predict(richiesta: Richiesta):\n",
        "\n",
        "      output = generator.generate_text (\n",
        "            prompt = richiesta.prompt,\n",
        "            length = richiesta.length,\n",
        "            stop_token = richiesta.stop_token,\n",
        "            temperature = richiesta.temperature,\n",
        "            repetition_penalty = richiesta.repetition_penalty,\n",
        "            k = richiesta.k,\n",
        "            p = richiesta.p\n",
        "        )\n",
        "      \n",
        "      return {\"result\": output}\n",
        "\n",
        "uvicorn.run(app,port=5000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [1196]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70UCuM70F1PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Codice vecchio per flask\n",
        "\"\"\"\n",
        "from flask import Flask, abort, jsonify, request\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "      prompt = request.args.get('prompt')\n",
        "      output = generate_text(\n",
        "            model_type='gpt2',\n",
        "            length=20,\n",
        "            prompt='hello world',\n",
        "            model_name_or_path='gpt2-large'\n",
        "        )\n",
        "      #abort(404, description=\"Resource not found\")\n",
        "      return jsonify({'result': output})\n",
        "\n",
        "@app.route(\"/predict\")\n",
        "def predict():\n",
        "    return \"no\"\n",
        "\n",
        "app.run();\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wsEs5uF0dEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#QUESTA CELLA è PERICOLOSA! FA CRASHARE COLAB COSì TI DA Più MEMORIA\n",
        "\"\"\"\n",
        "a = []\n",
        "while(1):\n",
        "    a.append('1')\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RG9wCOIX-3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PER VEDERE CHE SCHEDA GRAFICA MI è STATA ASSEGNATA (Tesla P100 PCIE è la migliore, Tesla K80 è la più schifosa)\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_qmzAVlYSds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#codice per montare il mio google drive\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\"\"\"\n",
        "\n",
        "#Prova quick & dirty del modello\n",
        "\"\"\"\n",
        "!wget https://raw.githubusercontent.com/huggingface/pytorch-transformers/master/examples/run_generation.py\n",
        "\n",
        "!python run_generation.py \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQz2HTs_aXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CODICE PER PREVENIRE IL RUNTIME DAL DISCONNETTERSI PER INATTIVITà:\n",
        "#ATTENZIONE: Potrebbe causare la revoca dei privilegi di COLAB se lo si lascia andare per troppo tempo!\n",
        "\n",
        "#function ClickConnect(){console.log(\"Working\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}