{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProgettoTipsit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/McLavish/telegramBot/blob/master/ProgettoTipsit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtwzZwAH0why",
        "colab_type": "text"
      },
      "source": [
        "# Programma WOW per generare testo con GTP-2 VELOCEMENTE :0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VfpPtOgFYGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dependency per il machine learning\n",
        "!pip install torch torchvision\n",
        "!pip install transformers\n",
        "\n",
        "#modelli disponibili: https://huggingface.co/models?search=gpt2-"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bw3ZvvWlVyQS",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "\n",
        "class GeneratoreTesto:\n",
        "    def __init__(self):\n",
        "      seed = random.randint(0, 2147483647)\n",
        "      torch.random.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "      #imposto la GPU come dispositivo da utilizzare (La CPU è lenta)\n",
        "      self.device = \"cuda\"\n",
        "\n",
        "      #lista modelli: https://huggingface.co/models?search=gpt2-\n",
        "      model_name_or_path = \"gpt2-xl\"\n",
        "\n",
        "      self.tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "      self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "      #imposto modalità evaluation del modello (no training)\n",
        "      self.model.eval()\n",
        "      self.model.to(self.device)\n",
        "      \n",
        "    def generate_text(self,\n",
        "      prompt=\"\",\n",
        "      #lunghezza generazione\n",
        "      length=20,\n",
        "      #se devo fermarmi prima a leggere il testo generato\n",
        "      stop_token = None,\n",
        "      temperature=1.0,\n",
        "      repetition_penalty=1.0,\n",
        "      k=0,\n",
        "      p=0.9,\n",
        "      #quanti campioni voglio generare (io 1 alla volta)\n",
        "      num_return_sequences=1,\n",
        "    ):\n",
        "      #Prendo la lunghezza massima del modello\n",
        "      lunghezza_massima = self.model.config.max_position_embeddings\n",
        "\n",
        "      print(lunghezza_massima);\n",
        "      \n",
        "      #Controllo che la lunghezza richiesta + la lunghezza dell'input siano minori del modello\n",
        "      if length < 0 or length > lunghezza_massima:\n",
        "        length = lunghezza_massima\n",
        "\n",
        "      #Codifico il testo\n",
        "      encoded_prompt = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "      encoded_prompt = encoded_prompt.to(self.device)\n",
        "\n",
        "      #Genero la sequenza\n",
        "      output_sequences = self.model.generate (\n",
        "          input_ids=encoded_prompt,\n",
        "          max_length=length + len(encoded_prompt[0]),\n",
        "          temperature=temperature,\n",
        "          top_k=k,\n",
        "          top_p=p,\n",
        "          repetition_penalty=repetition_penalty,\n",
        "          do_sample=True,\n",
        "          num_return_sequences=num_return_sequences,\n",
        "          )\n",
        "      \n",
        "      #Prendo solo il primo batch (potrei generare più batch in parallelo ma al mio programma non serve)\n",
        "      generated_sequence = output_sequences[0]\n",
        "\n",
        "      # Decodifico il testo\n",
        "      text = self.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "      # Rimuovo il prompt iniziale dal testo (perchè ti ridà anche quello dopo che ha generato)\n",
        "      total_sequence = (text[len(prompt) :])\n",
        "\n",
        "      # Se è stato specificato uno stop token rimuovo tutto quello che c'è dopo di lui\n",
        "      text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "      return total_sequence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVhEbFr2wiZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Istanzio l'oggetto che mi genera il testo e faccio una prova (così carica in memoria il modello)\n",
        "\n",
        "generator = GeneratoreTesto()\n",
        "\n",
        "generator.generate_text(\n",
        "            length=50,\n",
        "            prompt=\"\"\"Hamlet was once a man of honor\"\"\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW8O3qUGFp9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creo un tunnel con ngrok perchè Google Colab non permette la creazione di API esposte\n",
        "\n",
        "!if [ ! -f \"ngrok-stable-linux-amd64.zip\" ]; then wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip; fi\n",
        "!if [ ! -f \"ngrok\" ]; then unzip -o ngrok-stable-linux-amd64.zip; fi\n",
        "\n",
        "!./ngrok authtoken \"1aMSPEPk5WpPiquwebio1IWcILq_2RRMqrTUj968qnmrXss6L\"\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 5000 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AhLh5NU6uxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dependency per il web server con FastAPI\n",
        "\n",
        "!pip install fastapi\n",
        "!pip install uvicorn\n",
        "!pip install pydantic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnpHZhGk24Ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "class Richiesta(BaseModel):\n",
        "    prompt: str\n",
        "    length: int = None\n",
        "    stop_token: str = None\n",
        "    temperature: float = None\n",
        "    repetition_penalty: float = None\n",
        "    k: float = None\n",
        "    p: float = None\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "      return {\"string\": \"Hello World\"}\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "def predict(richiesta: Richiesta):\n",
        "\n",
        "      output = generator.generate_text (\n",
        "            prompt = richiesta.prompt,\n",
        "            length = richiesta.length,\n",
        "            stop_token = richiesta.stop_token,\n",
        "            temperature = richiesta.temperature,\n",
        "            repetition_penalty = richiesta.repetition_penalty,\n",
        "            k = richiesta.k,\n",
        "            p = richiesta.p\n",
        "        )\n",
        "      \n",
        "      return {\"result\": output}\n",
        "\n",
        "uvicorn.run(app,port=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70UCuM70F1PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Codice vecchio per flask\n",
        "\"\"\"\n",
        "from flask import Flask, abort, jsonify, request\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "      prompt = request.args.get('prompt')\n",
        "      output = generate_text(\n",
        "            model_type='gpt2',\n",
        "            length=20,\n",
        "            prompt='hello world',\n",
        "            model_name_or_path='gpt2-large'\n",
        "        )\n",
        "      #abort(404, description=\"Resource not found\")\n",
        "      return jsonify({'result': output})\n",
        "\n",
        "@app.route(\"/predict\")\n",
        "def predict():\n",
        "    return \"no\"\n",
        "\n",
        "app.run();\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wsEs5uF0dEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#QUESTA CELLA è PERICOLOSA! FA CRASHARE COLAB COSì TI DA Più MEMORIA\n",
        "\"\"\"\n",
        "a = []\n",
        "while(1):\n",
        "    a.append('1')\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RG9wCOIX-3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PER VEDERE CHE SCHEDA GRAFICA MI è STATA ASSEGNATA (Tesla P100 PCIE è la migliore, Tesla K80 è la più schifosa)\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_qmzAVlYSds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#codice per montare il mio google drive\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\"\"\"\n",
        "\n",
        "#Prova quick & dirty del modello\n",
        "\"\"\"\n",
        "!wget https://raw.githubusercontent.com/huggingface/pytorch-transformers/master/examples/run_generation.py\n",
        "\n",
        "!python run_generation.py \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQz2HTs_aXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CODICE PER PREVENIRE IL RUNTIME DAL DISCONNETTERSI PER INATTIVITà:\n",
        "#ATTENZIONE: Potrebbe causare la revoca dei privilegi di COLAB se lo si lascia andare per troppo tempo!\n",
        "\n",
        "#function ClickConnect(){console.log(\"Working\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}